{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code is accompany the manuscript \"Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent\" (2020) by D. Richards and P. Rebeschini. \n",
    "\n",
    "It presents a simple set of functions for investigating the generalisation performance of Distributed Gradient Descent applied to Logistic regression with simulated data. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "import matplotlib.ticker as mticker\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "#Make this notebook wider\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for generating simulated dat from a Logistic Regression\n",
    "'''\n",
    "def GeneratedData(DataParams):\n",
    "    np.random.seed(DataParams[\"Seed\"])\n",
    "    OutputDict = {}\n",
    "    OutputDict[\"DataParams\"] = copy.deepcopy(DataParams)\n",
    "    print(\"Generating Training Data..\") \n",
    "    \n",
    "    OutputDict[\"X_Train\"] =  np.random.normal(size = (DataParams[\"Dimension\"],DataParams[\"TotalSampleSize\"]))\n",
    "    OutputDict[\"X_Train\"] /= np.sqrt((OutputDict[\"X_Train\"] ** 2 ).sum(axis=0)) \n",
    "    print(\"Generating Test Data...\")\n",
    "    \n",
    "    OutputDict[\"X_Test\"]=  np.random.normal(size = (DataParams[\"Dimension\"],DataParams[\"TestSize\"]))\n",
    "    OutputDict[\"X_Test\"] /= np.sqrt((OutputDict[\"X_Test\"] ** 2 ).sum(axis=0))\n",
    "    \n",
    "    #True parameter\n",
    "    OutputDict[\"ThetaTrue\"] = np.random.normal(size=(DataParams[\"Dimension\"],1))\n",
    "    OutputDict[\"ThetaTrue\"][0:int(np.sqrt(DataParams[\"Dimension\"]))] = 0.\n",
    "    \n",
    "    OutputDict[\"Y_Train\"] = np.sign(OutputDict[\"ThetaTrue\"].T.dot(OutputDict[\"X_Train\"]) + np.random.normal(size= (1,DataParams[\"TotalSampleSize\"]),scale=DataParams[\"NoiseSD\"]))[0]\n",
    "    OutputDict[\"Y_Test\"]  = np.sign(OutputDict[\"ThetaTrue\"].T.dot(OutputDict[\"X_Test\"])  + np.random.normal(size= (1,DataParams[\"TestSize\"]),scale=DataParams[\"NoiseSD\"]))[0]\n",
    "    \n",
    "    print(\"Fitting Logistic Regression\")\n",
    "    #Calculate minimiser of empriical loss, \n",
    "    clf = LogisticRegression(C=np.inf,fit_intercept=False,solver=\"lbfgs\",tol=1e-15,max_iter=1000)\n",
    "    clf.fit(OutputDict[\"X_Train\"].T,OutputDict[\"Y_Train\"])\n",
    "    OutputDict[\"FittedCoeff\"] = clf.coef_\n",
    "    TmpConst = np.exp(-OutputDict[\"Y_Train\"]*OutputDict[\"FittedCoeff\"].dot(OutputDict[\"X_Train\"]))\n",
    "    OutputDict[\"FittedLoss\"] = np.log(1. + TmpConst).mean() \n",
    "    \n",
    "    #Calculate smoothness and Lipschitz constant\n",
    "    OutputDict[\"R\"] = np.sqrt((OutputDict[\"FittedCoeff\"]**2).sum())\n",
    "    OutputDict[\"Beta\"] = 0.\n",
    "    for j in np.arange(OutputDict[\"X_Train\"].shape[1]):\n",
    "        if(OutputDict[\"X_Train\"][:,j].dot(OutputDict[\"X_Train\"][:,j]) > OutputDict[\"Beta\"]):\n",
    "            OutputDict[\"Beta\"] = OutputDict[\"X_Train\"][:,j].dot(OutputDict[\"X_Train\"][:,j])\n",
    "    OutputDict[\"Beta\"] /= 4.0\n",
    "    OutputDict[\"L\"] = np.sqrt((OutputDict[\"X_Train\"]**2).sum(axis=0)).mean()\n",
    "    return(OutputDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for performing Distributed Gradient Descent\n",
    "\n",
    "Inputs:\n",
    "    DataSet: Output from GenerateData \n",
    "    OptParams: Collection of optimisation parameters\n",
    "'''\n",
    "def DistributedGradientDescent(DataSet,OptParams):\n",
    "    OutputDict = {}\n",
    "    #Construct the gossip matrix, given the network type and number of agents.\n",
    "    if(OptParams[\"NetworkType\"] == \"Grid\"):\n",
    "        TmpGridDim = int(np.sqrt(OptParams[\"NodeCount\"]))\n",
    "        OutputDict[\"Network\"] = nx.grid_graph([TmpGridDim,TmpGridDim])\n",
    "    elif(OptParams[\"NetworkType\"] == \"Cycle\"):\n",
    "        OutputDict[\"Network\"] = nx.path_graph(OptParams[\"NodeCount\"])\n",
    "    elif(OptParams[\"NetworkType\"] == \"Complete\"):\n",
    "        OutputDict[\"Network\"] = nx.complete_graph(OptParams[\"NodeCount\"])\n",
    "\n",
    "    OutputDict[\"AdjMatrix\"] = np.array(nx.adj_matrix(OutputDict[\"Network\"]).todense())\n",
    "    OutputDict[\"Degrees\"] = OutputDict[\"AdjMatrix\"].sum(axis=0)\n",
    "    OutputDict[\"GossipMatrix\"] = np.diag(1. / OutputDict[\"Degrees\"] ).dot(OutputDict[\"AdjMatrix\"])\n",
    "    \n",
    "    MaxDeg = OutputDict[\"Degrees\"].max()\n",
    "    mixprob = MaxDeg/(MaxDeg + 1.)\n",
    "    OutputDict[\"GossipMatrix\"]  = ( 1. - mixprob ) * np.identity(OptParams[\"NodeCount\"]) + mixprob * OutputDict[\"GossipMatrix\"]\n",
    "    \n",
    "    #Compute the spectral gap of the gossip matrix\n",
    "    OutputDict[\"AbsEigenvals\"] = np.abs(np.linalg.eigvals(OutputDict[\"GossipMatrix\"]))\n",
    "    if(OptParams[\"NetworkType\"] is not  \"Complete\"):\n",
    "        OutputDict[\"SpectralGap\"]  = np.sqrt(1./(1. - OutputDict[\"AbsEigenvals\"][np.argsort(-OutputDict[\"AbsEigenvals\"])[1]]))\n",
    "    else:\n",
    "        OutputDict[\"SpectralGap\"] = 1.\n",
    "    \n",
    "    #Distribute data between the nodes. Shuffle first\n",
    "    ShuffledIndexes = np.random.choice(np.arange(DataSet[\"DataParams\"][\"TotalSampleSize\"]),replace=False,size=DataSet[\"DataParams\"][\"TotalSampleSize\"])\n",
    "    OutputDict[\"CollectedIndexes\"] = []\n",
    "    OutputDict[\"SampleSizePerAgent\"] = DataSet[\"DataParams\"][\"TotalSampleSize\"] / OptParams[\"NodeCount\"]\n",
    "    if( OutputDict[\"SampleSizePerAgent\"] - int(OutputDict[\"SampleSizePerAgent\"]) != 0.):\n",
    "        print(\"Samples Do not Divide Evenly\")\n",
    "        return()\n",
    "    OutputDict[\"SampleSizePerAgent\"] = int(DataSet[\"DataParams\"][\"TotalSampleSize\"] / OptParams[\"NodeCount\"])\n",
    "    for i in np.arange(OptParams[\"NodeCount\"]):\n",
    "        OutputDict[\"CollectedIndexes\"].append(ShuffledIndexes[(i * OutputDict[\"SampleSizePerAgent\"]  ):((i+1) * OutputDict[\"SampleSizePerAgent\"] )])\n",
    "    \n",
    "    #Choose the step size - potentially depending on the spectral gap. Indexed from 1 - 5\n",
    "    if(OptParams[\"StepSizeChoice\"] == 1):\n",
    "        OutputDict[\"LearningRate\"] = DataSet[\"R\"]/(DataSet[\"L\"]*np.sqrt(OptParams[\"Iterations\"]))\n",
    "    elif(OptParams[\"StepSizeChoice\"] == 2):\n",
    "        OutputDict[\"LearningRate\"] = DataSet[\"R\"]/(DataSet[\"L\"]*np.sqrt(OptParams[\"Iterations\"]) *  np.sqrt( OutputDict[\"SpectralGap\"] * OutputDict[\"SpectralGap\"]))\n",
    "    elif(OptParams[\"StepSizeChoice\"] == 3):\n",
    "        OutputDict[\"LearningRate\"] = DataSet[\"R\"]/(DataSet[\"L\"]*np.sqrt(OptParams[\"Iterations\"]) *  np.sqrt( OutputDict[\"SpectralGap\"] * OutputDict[\"SpectralGap\"] + (float(OptParams[\"Iterations\"]) / float(DataSet[\"DataParams\"][\"TotalSampleSize\"])  ) ))\n",
    "    elif(OptParams[\"StepSizeChoice\"] == 4):\n",
    "        OutputDict[\"LearningRate\"] = DataSet[\"R\"]  /(DataSet[\"L\"]* np.sqrt( DataSet[\"DataParams\"][\"TotalSampleSize\"]))\n",
    "    elif(OptParams[\"StepSizeChoice\"] == 5):\n",
    "        OutputDict[\"LearningRate\"] = DataSet[\"R\"] /(DataSet[\"L\"]*  OutputDict[\"SpectralGap\"] * OutputDict[\"SpectralGap\"] * np.sqrt( DataSet[\"DataParams\"][\"TotalSampleSize\"]))\n",
    "    OutputDict[\"LearningRate\"] *= OptParams[\"LearningRateScaling\"]\n",
    "    OutputDict[\"LearningRate\"] = 1./(1./4. + 1./OutputDict[\"LearningRate\"])\n",
    "    \n",
    "    #Storage for Optimisation, Population Errors. Evaluate on a log scale, for a total of 1000 points.  \n",
    "    OutputDict[\"OptError\"] = []\n",
    "    OutputDict[\"PopError\"] = []\n",
    "    OutputDict[\"ErrorEvaluatePoints\"] = np.unique((10**(np.linspace(0,np.log10(OptParams[\"Iterations\"]),1000))).astype(\"int\")) -1 \n",
    "    \n",
    "    #Begin optimisation. Initialise parameters.\n",
    "    OutputDict[\"Parameters\"] = np.zeros((OptParams[\"NodeCount\"],DataSet[\"DataParams\"][\"Dimension\"]))\n",
    "    OutputDict[\"AvgParameters\"] = copy.deepcopy(OutputDict[\"Parameters\"])\n",
    "    for t in np.arange(OptParams[\"Iterations\"]):\n",
    "        if(t % int(OptParams[\"Iterations\"]*0.1) == 0):\n",
    "            print(str(t + 1) + \" / \" + str(OptParams[\"Iterations\"]))\n",
    "        \n",
    "        #Iterate through agents, performing a gradient descent step.  \n",
    "        for i in np.arange(OptParams[\"NodeCount\"]):\n",
    "            SampledIndex = np.random.choice(OutputDict[\"CollectedIndexes\"][i])\n",
    "            X_Sampled = DataSet[\"X_Train\"][:,SampledIndex]\n",
    "            Y_Sampled = DataSet[\"Y_Train\"][SampledIndex]\n",
    "            \n",
    "            TmpConst = np.exp(-Y_Sampled *  OutputDict[\"Parameters\"][i].dot(X_Sampled))\n",
    "            Denomin = 1. + TmpConst\n",
    "            TmpGradient = -TmpConst*Y_Sampled*(X_Sampled/Denomin)\n",
    "            OutputDict[\"Parameters\"][i] = OutputDict[\"Parameters\"][i] - OutputDict[\"LearningRate\"] * TmpGradient\n",
    "        \n",
    "        #Gossip gradients to neighbours and compute rolling average\n",
    "        OutputDict[\"Parameters\"] = OutputDict[\"GossipMatrix\"].dot(OutputDict[\"Parameters\"])\n",
    "        OutputDict[\"AvgParameters\"] = float(t) * OutputDict[\"AvgParameters\"] /float(t+1) + OutputDict[\"Parameters\"]/float(t+1)\n",
    "        \n",
    "        #compute the error - in this case if we have chosen step size 4 or 5. \n",
    "        if(( OptParams[\"StepSizeChoice\"] in [4,5]  and t in OutputDict[\"ErrorEvaluatePoints\"] ) or ( OptParams[\"StepSizeChoice\"] not in [4,5] and  t == OptParams[\"Iterations\"]-1) ):\n",
    "            TmpOptError = 0.\n",
    "            TmpPopError = 0.\n",
    "            for i in np.arange(OptParams[\"NodeCount\"]):\n",
    "                TmpOptError += np.log( 1. + np.exp(- DataSet[\"Y_Train\"][OutputDict[\"CollectedIndexes\"][i]]  *  OutputDict[\"AvgParameters\"][i].dot(DataSet[\"X_Train\"][:,OutputDict[\"CollectedIndexes\"][i]])) ).sum()\n",
    "                TmpPopError_2 =  np.log( 1. + np.exp( - DataSet[\"Y_Test\"]  *  OutputDict[\"AvgParameters\"][i].dot( DataSet[\"X_Test\"] ) ) ).mean()\n",
    "                if(TmpPopError_2 > TmpPopError):\n",
    "                    TmpPopError = copy.deepcopy(TmpPopError_2)\n",
    "            TmpOptError = TmpOptError/DataSet[\"DataParams\"][\"TotalSampleSize\"]\n",
    "            OutputDict[\"OptError\"].append(TmpOptError)\n",
    "            OutputDict[\"PopError\"].append(TmpPopError)\n",
    "    return(OutputDict)\n",
    "\n",
    "'''\n",
    "Function for performing centralised Gradient descent. Similar to Distributed Gradient Descent function. \n",
    "'''\n",
    "def CentralisedGradientDescent(DataSet,OptParams):\n",
    "    OutputDict = {}\n",
    "    OutputDict[\"LearningRate\"] = DataSet[\"R\"]  /(DataSet[\"L\"]* np.sqrt( DataSet[\"DataParams\"][\"TotalSampleSize\"]))\n",
    "    OutputDict[\"LearningRate\"] = DataSet[\"R\"]  /(DataSet[\"L\"]* np.sqrt( OptParams[\"Iterations\"]))\n",
    "    OutputDict[\"LearningRate\"] *= OptParams[\"LearningRateScaling\"]\n",
    "    OutputDict[\"LearningRate\"] = 1./(1./4. + 1./OutputDict[\"LearningRate\"])\n",
    "    OutputDict[\"OptError\"] = []\n",
    "    OutputDict[\"PopError\"] = []\n",
    "    OutputDict[\"ErrorEvaluatePoints\"] = np.unique((10**(np.linspace(0,np.log10(OptParams[\"Iterations\"]),1000))).astype(\"int\")) - 1 \n",
    "    OutputDict[\"Parameters\"] = np.zeros(DataSet[\"DataParams\"][\"Dimension\"])\n",
    "    OutputDict[\"AvgParameters\"] = copy.deepcopy(OutputDict[\"Parameters\"])\n",
    "    for t in np.arange(OptParams[\"Iterations\"]):\n",
    "        SampledIndex = np.random.choice(DataSet[\"DataParams\"][\"TotalSampleSize\"])\n",
    "        X_Sampled = DataSet[\"X_Train\"][:,SampledIndex]\n",
    "        Y_Sampled = DataSet[\"Y_Train\"][SampledIndex]\n",
    "        TmpConst = np.exp(-Y_Sampled *  OutputDict[\"Parameters\"].dot(X_Sampled))\n",
    "        Denomin = 1. + TmpConst\n",
    "        TmpGradient = -TmpConst*Y_Sampled*(X_Sampled/Denomin)\n",
    "        OutputDict[\"Parameters\"] = OutputDict[\"Parameters\"] - OutputDict[\"LearningRate\"] * TmpGradient\n",
    "        OutputDict[\"AvgParameters\"] = float(t) * OutputDict[\"AvgParameters\"] /float(t+1) + OutputDict[\"Parameters\"]/float(t+1)\n",
    "        if(t in OutputDict[\"ErrorEvaluatePoints\"]): # \n",
    "            TmpOptError = np.log( 1. + np.exp(- DataSet[\"Y_Train\"]  *  OutputDict[\"AvgParameters\"].dot(DataSet[\"X_Train\"]) ) ).sum()\n",
    "            TmpPopError = np.log( 1. + np.exp( - DataSet[\"Y_Test\"]  *  OutputDict[\"AvgParameters\"].dot(DataSet[\"X_Test\"] ) ) ).mean()\n",
    "            TmpOptError = TmpOptError / DataSet[\"DataParams\"][\"TotalSampleSize\"]\n",
    "            OutputDict[\"OptError\"].append(TmpOptError)\n",
    "            OutputDict[\"PopError\"].append(TmpPopError)\n",
    "    return(OutputDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Example of running Distributed Gradient Descent function with a collection of different parameters. \n",
    "'''\n",
    "\n",
    "DataParams = {\"Seed\":12525,\n",
    "             \"Dimension\": 100,\n",
    "            \"TotalSampleSize\":25*100,\n",
    "             \"NoiseSD\":0.,\n",
    "            \"TestSize\":1000}\n",
    "OptimisationParameters = {\"NetworkType\":\"Grid\",\n",
    "                          \"NodeCount\":100,\n",
    "                         \"Iterations\":1000,\n",
    "                         \"StepSizeChoice\":1,\n",
    "                         \"LearningRateScaling\": 1. }\n",
    "#Generate Data \n",
    "Data = GeneratedData(DataParams)\n",
    "\n",
    "\n",
    "#Different Graph topologies\n",
    "GraphTopologies = [\"Grid\",\"Complete\"] \n",
    "\n",
    "# Various step size codes- see function DistributedGradientDescent\n",
    "StepSizeIndexes = [1,2,3,4,5]\n",
    "\n",
    "#Total number of iterations\n",
    "IterationsList = [10**5,10**7]\n",
    "\n",
    "#To store output and spectral gap\n",
    "ResultsDict = {}\n",
    "SpectralGapStore = {}\n",
    "\n",
    "#Iterate through all combinations of experimental parameters\n",
    "for k,t_iters in enumerate(IterationsList):\n",
    "    ResultsDict[str(t_iters)] = {}\n",
    "    print(\"t = \" + str(t_iters) + \" (\" + str(k+1) + \"/\" + str(len(IterationsList)) + \")\")\n",
    "    for stepsize_iter in StepSizeIndexes:\n",
    "        ResultsDict[str(t_iters)][str(stepsize_iter)] = {}\n",
    "        for graphtype_iter in GraphTopologies:\n",
    "            #Set Optimisation parameters\n",
    "            OptimisationParameters[\"Iterations\"] = t_iters\n",
    "            OptimisationParameters[\"NetworkType\"] = graphtype_iter\n",
    "            OptimisationParameters[\"StepSizeChoice\"] = stepsize_iter\n",
    "            \n",
    "            #Time and run the experiment\n",
    "            start = time.time()\n",
    "            TmpResults = DistributedGradientDescent(Data,OptimisationParameters)\n",
    "            end = time.time()\n",
    "            print(\"Runtime:\" + str(end - start))\n",
    "\n",
    "            #Extract required quantities\n",
    "            SpectralGapStore[graphtype_iter] = TmpResults[\"SpectralGap\"]\n",
    "            ResultsDict[str(t_iters)][str(stepsize_iter)][str(graphtype_iter)] = copy.deepcopy(TmpResults)\n",
    "            print(\"  StepSize Type:\" + str(stepsize_iter) + \"  Graph Type: \" + str(graphtype_iter) + \"  OptError: \" + str(TmpResults[\"OptError\"][-1]) + \"  PopError:\" + str(TmpResults[\"PopError\"][-1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
